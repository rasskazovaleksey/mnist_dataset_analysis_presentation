{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.model_processor import read_params\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def print_params(model: str):\n",
    "    models_params = read_params()\n",
    "    model = next(obj for obj in models_params if obj[\"model\"] == model)\n",
    "    enumeration_count = 1\n",
    "    for key, value in model[\"params\"].items():\n",
    "        enumeration_count *= len(value)\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\")\n",
    "    print(f\"Total number of combitations: {enumeration_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "logistic_regression"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty: [None, 'l2']\n",
      "solver: ['lbfgs', 'sag', 'saga']\n",
      "C: [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
      "max_iter: [10000]\n",
      "\n",
      "Total number of combitations: 36\n"
     ]
    }
   ],
   "source": [
    "print_params(\"LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "k_neighbors_classifier"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neighbors: [1, 2, 3, 5, 8, 13, 21]\n",
      "p: [1, 2]\n",
      "weights: ['uniform', 'distance']\n",
      "\n",
      "Total number of combitations: 28\n"
     ]
    }
   ],
   "source": [
    "print_params(\"KNeighborsClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "gaussianNB"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of combitations: 1\n"
     ]
    }
   ],
   "source": [
    "print_params(\"GaussianNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "DecisionTreeClassifier"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion: ['gini', 'entropy', 'log_loss']\n",
      "splitter: ['best', 'random']\n",
      "max_depth: [None, 10, 20, 30, 50, 100, 200]\n",
      "\n",
      "Total number of combitations: 42\n"
     ]
    }
   ],
   "source": [
    "print_params(\"DecisionTreeClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "RandomForestClassifier"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion: ['gini', 'entropy', 'log_loss']\n",
      "max_depth: [None, 10, 20, 30, 50, 100, 200]\n",
      "\n",
      "Total number of combitations: 21\n"
     ]
    }
   ],
   "source": [
    "print_params(\"RandomForestClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "SVC"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
      "kernel: ['poly', 'rbf', 'sigmoid']\n",
      "\n",
      "Total number of combitations: 18\n"
     ]
    }
   ],
   "source": [
    "print_params(\"SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.scaler import min_max_scaler\n",
    "from libs.scaler import black_and_white_scaler\n",
    "from libs.mnist_reader import load_mnist\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train_mnist, Y_train_mnist = load_mnist(path=\"data/mnist\", kind=\"train\")\n",
    "X_train_fashion, Y_train_fashion = load_mnist(path=\"data/fashion\", kind=\"train\")\n",
    "\n",
    "mnist_black_and_white_no_scaling = black_and_white_scaler(X_train_mnist)\n",
    "fashion_black_and_white_no_scaling = black_and_white_scaler(X_train_fashion)\n",
    "\n",
    "datasets = {\n",
    "    # mnist\n",
    "    \"mnist_original\": X_train_mnist,\n",
    "    \"mnist_black_and_white_no_scaling\": mnist_black_and_white_no_scaling,\n",
    "    \"mnist_original_89_attributes\": PCA(n_components=89).fit_transform(X_train_mnist),\n",
    "    \"mnist_black_and_white_233_attributes\": PCA(n_components=233).fit_transform(mnist_black_and_white_no_scaling),\n",
    "    # fashion\n",
    "    \"fashion_original\": X_train_fashion,\n",
    "    \"fashion_black_and_white_no_scaling\": fashion_black_and_white_no_scaling,\n",
    "    \"fashion_original_89_attributes\": PCA(n_components=89).fit_transform(X_train_fashion),\n",
    "    \"fashion_black_and_white_233_attributes\": PCA(n_components=233).fit_transform(fashion_black_and_white_no_scaling),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define single iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "single_iteration"
    ]
   },
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "#| label: single_iteration\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from libs.model_processor import create_default_model\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def iterate_model(model_dict: dict, X: list, y: list, dataset_name: str):\n",
    "    Path(\"output\").mkdir(parents=True, exist_ok=True)\n",
    "    model = create_default_model(model_dict[\"model\"])\n",
    "    result_file = f\"output/{dataset_name}_{model_dict['model']}.csv\"\n",
    "    if Path(result_file).is_file():\n",
    "        print(f\"Skipping '{result_file}' since in alredy exist\")\n",
    "        return\n",
    "    \n",
    "    clf = GridSearchCV(estimator=model, param_grid=model_dict[\"params\"], verbose=3, cv=[(slice(None), slice(None))])\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df.to_csv(result_file)\n",
    "\n",
    "    return clf.cv_results_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset mnist_original and LogisticRegression model, see _05_modeling.py\n",
      "For dataset mnist_original and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset mnist_original and GaussianNB model, see _05_modeling.py\n",
      "For dataset mnist_original and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset mnist_original and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset mnist_original and SVC model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_no_scaling and LogisticRegression model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_no_scaling and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_no_scaling and GaussianNB model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_no_scaling and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_no_scaling and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_no_scaling and SVC model, see _05_modeling.py\n",
      "For dataset mnist_original_89_attributes and LogisticRegression model, see _05_modeling.py\n",
      "For dataset mnist_original_89_attributes and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset mnist_original_89_attributes and GaussianNB model, see _05_modeling.py\n",
      "For dataset mnist_original_89_attributes and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset mnist_original_89_attributes and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset mnist_original_89_attributes and SVC model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_233_attributes and LogisticRegression model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_233_attributes and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_233_attributes and GaussianNB model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_233_attributes and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_233_attributes and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset mnist_black_and_white_233_attributes and SVC model, see _05_modeling.py\n",
      "For dataset fashion_original and LogisticRegression model, see _05_modeling.py\n",
      "For dataset fashion_original and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset fashion_original and GaussianNB model, see _05_modeling.py\n",
      "For dataset fashion_original and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset fashion_original and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset fashion_original and SVC model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_no_scaling and LogisticRegression model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_no_scaling and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_no_scaling and GaussianNB model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_no_scaling and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_no_scaling and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_no_scaling and SVC model, see _05_modeling.py\n",
      "For dataset fashion_original_89_attributes and LogisticRegression model, see _05_modeling.py\n",
      "For dataset fashion_original_89_attributes and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset fashion_original_89_attributes and GaussianNB model, see _05_modeling.py\n",
      "For dataset fashion_original_89_attributes and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset fashion_original_89_attributes and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset fashion_original_89_attributes and SVC model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_233_attributes and LogisticRegression model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_233_attributes and KNeighborsClassifier model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_233_attributes and GaussianNB model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_233_attributes and DecisionTreeClassifier model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_233_attributes and RandomForestClassifier model, see _05_modeling.py\n",
      "For dataset fashion_black_and_white_233_attributes and SVC model, see _05_modeling.py\n"
     ]
    }
   ],
   "source": [
    "models_params = read_params()\n",
    "# for each dataset\n",
    "for key in datasets:\n",
    "    # for each model in dict\n",
    "    for model_dict in models_params:\n",
    "        X = datasets[key]\n",
    "        if key.startswith(\"mnist\"):\n",
    "            y = Y_train_mnist\n",
    "        elif key.startswith(\"fashion\"):\n",
    "            y = Y_train_fashion\n",
    "        else:\n",
    "            raise KeyError\n",
    "        print(f\"For dataset {key} and {model_dict['model']} model, see _05_modeling.py\")\n",
    "        # iterate_model(model_dict, X=X, y=y, dataset_name=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "time"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 146 tested models.\n",
      "Overall time to fit models: 5 days, 22:09:13\n",
      "Overall time to score models: 1 day, 23:06:06\n",
      "\n",
      "Total time: 7 days, 21:15:19\n"
     ]
    }
   ],
   "source": [
    "s\n",
    "\n",
    "df = pd.concat(map(pd.read_csv, filenames), ignore_index=True) \n",
    "\n",
    "print(f\"For {len(df)} tested models.\")\n",
    "total_fit_time = int(df[\"mean_fit_time\"].sum())\n",
    "total_score_time = int(df[\"mean_score_time\"].sum())\n",
    "total_time = total_fit_time + total_score_time\n",
    "print(f\"Overall time to fit models: {datetime.timedelta(seconds = total_fit_time)}\")\n",
    "print(f\"Overall time to score models: {datetime.timedelta(seconds = total_score_time)}\")\n",
    "print()\n",
    "print(f\"Total time: {datetime.timedelta(seconds = total_time)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
